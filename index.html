<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting</h2>
            <h4 style="color:#5a6268;">CVPR 2023</h4>
            <hr>
            <h6> <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=YAdDCr0AAAAJ" target="_blank">Haiping Wang</a><sup>*,1</sup>, 
                <a href="https://liuyuan-pal.github.io/" target="_blank">Yuan Liu</a><sup>*,2</sup>,
                <a href="https://dongzhenwhu.github.io/" target="_blank">Zhen Dong</a><sup>&dagger;,1</sup>, 
                <a href="http://yulanguo.me/" target="_blank">Yulan Guo</a><sup>3</sup>,
                <a href="http://yulanguo.me/" target="_blank">Yushen Liu</a><sup>4</sup>,
                <a href="https://www.cs.hku.hk/people/academic-staff/wenping" target="_blank">Wenping Wang</a><sup>5</sup>
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=DZsF2oIAAAAJ" target="_blank">Bisheng Yang</a><sup>&dagger;,1</sup> <br>
            
              <p>
              <sup>1</sup>Wuhan University &nbsp;&nbsp; 
              <sup>2</sup>The University of Hong Kong &nbsp;&nbsp; 
              <sup>3</sup>Sun Yat-sen University &nbsp;&nbsp; 
              <sup>4</sup>Tsinghua University &nbsp;&nbsp; 
              <sup>5</sup>Texas A&M University &nbsp;&nbsp; <br>

              <sup>*</sup>The first two authors contribute equally. &nbsp;&nbsp; 
              <sup>&dagger;</sup>Corresponding authors. &nbsp;&nbsp; 
              </p>
            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/WHU-USI3DV/SGHR" role="button"  target="_blank">
                    <i class="fa fa-github-alt"></i> Code</a> </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">  </p>
          <p class="text-justify"> In this paper, we present a new method for the multiview registration of point cloud.Previous multiview registration
             methods rely on exhaustive pairwise registration to construct a densely-connected pose graph and apply Iteratively Reweighted Least Square (IRLS) 
             on the pose graph to compute the scan poses. However, constructing a densely-connected graph is time-consuming and contains lots of outlier edges, 
             which makes the subsequent IRLS struggle to find correct poses. To address the above problems, we first propose to use a neural network to estimate
              the overlap between scan pairs, which enables us to construct a sparse but reliable pose graph. Then, we design a novel history reweighting 
              function in the IRLS scheme, which has strong robustness to outlier edges on the graph. In comparison with existing multiview registration methods, 
              our method achieves $11$\% higher registration recall on the 3DMatch dataset and $\sim13$\% lower registration errors on the ScanNet dataset while 
              reducing $\sim70$\% required pairwise registrations. Comprehensive ablation studies are conducted to demonstrate the effectiveness of our designs. </p>
          <hr style="margin-top:0px">
            <figure align="center"></figure>
              <p align="center"><img src="./sghr/pipeline.png" width="1100"/>
              </p>
            </figure>
              <!-- <br><br> -->
          <p class="text-justify">  </p>
          <p class="text-caption"> Fig.1 Pipeline of SGHR. Given $N$ unaligned partial scans (1), our target is to register all these scans into a completed point cloud (4). 
            SGHR has two contributions. (2) We learn a global feature vector to initialize a sparse pose graph which contains much less outliers and reduces the required number of pairwise registrations.
            (3) We propose a novel IRLS scheme. In our IRLS scheme, we initialize weights from both global features and pairwise registrations. Then, we design a history reweighting function to iteratively refine poses, which improves the robustness to outliers.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Qualitative Results -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Qualitative Results</h3>
            <hr style="margin-top:0px">
            <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">Selected multiview registration results on 3DMatch/ScanNet/ETH</h4>
              <figure align="center"></figure>
              <p align="center"><img src="./sghr/qualitive.png" width="1100"/>
              </p>
            </figure>

            <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">Comparison on 3DMatch and ETH</h4>
              <figure align="center"></figure>
              <p align="center"><img src="./sghr/3dmatch.png" width="800"/>
              </p>
            </figure>

            <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">Comparison on ScanNet</h4>
              <figure align="center"></figure>
              <p align="center"><img src="./sghr/scannet.png" width="800"/>
              </p>
            </figure>

        <p class="text-justify">  </p>
        </div>
    </div>
  </section>
  <br>

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
<pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>
}</code></pre>
          <hr>
      </div>
    </div>
  </div>

    <!-- others -->
    <div class="container">
      <div class="row ">
        <div class="col-12">
            <h3>Other Projects</h3>
            <hr style="margin-top:0px">
            <p class="text-justify"> Welcome to take a look at the homepage of our research group <a href="https://github.com/WHU-USI3DV" target="_blank">WHU-USI3DV</a> ! We focus on 3D Computer Vision, particularly including 3D reconstruction, scene understanding, point cloud processing as well as their applications in intelligent transportation system, digital twin cities, urban sustainable development, and robotics.</p>
            <hr>
        </div>
      </div>
    </div>

  <footer class="text-center" style="margin-bottom:10px">
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
  </footer>

</body>
</html>
